{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hQ8zhyAZwWIw",
        "outputId": "502990c1-9e54-4789-a42a-5cb509438c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting arabic_reshaper\n",
            "  Downloading arabic_reshaper-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
            "Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, arabic_reshaper, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5 arabic_reshaper-3.0.0 python-bidi-0.6.6\n"
          ]
        }
      ],
      "source": [
        "pip install PyMuPDF arabic_reshaper python-bidi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# --- Configuration ---\n",
        "MIN_PARAGRAPH_WORD_COUNT_INITIAL = 8  # Seuil pour qu'un bloc soit considéré après filtrage initial\n",
        "MIN_FUSED_PARAGRAPH_WORD_COUNT = 15     # Seuil final pour un paragraphe après fusion\n",
        "FUSE_CURRENT_PARA_MAX_WORDS = 35\n",
        "FUSE_NEXT_PARA_MAX_WORDS = 25\n",
        "\n",
        "PDF_DIRECTORY = \"/content/pdfs_arabes_test\"\n",
        "OUTPUT_CONTEXT_FILE = \"contextes_manuels_scolaires_v7_refined.jsonl\"\n",
        "\n",
        "# --- Fonctions de Nettoyage et Normalisation ---\n",
        "def normalize_arabic_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
        "    text = re.sub(r\"[إأآٱ]\", \"ا\", text)\n",
        "    text = re.sub(r\"ى\", \"ي\", text)\n",
        "    text = re.sub(r\"ـ\", '', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    # Les \\n sont traités séparément pour la structure des paragraphes\n",
        "    return text.strip() # Strip initial pour la fonction de filtrage\n",
        "\n",
        "def clean_internal_newlines_and_final_spaces(text_block):\n",
        "    if not isinstance(text_block, str): return \"\"\n",
        "    cleaned_text = text_block.replace(\"\\n\", \" \")\n",
        "    cleaned_text = re.sub(r' +', ' ', cleaned_text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def is_narrative_paragraph(text_paragraph_candidate):\n",
        "    text_stripped = text_paragraph_candidate.strip() # Travaille sur le texte déjà strippé\n",
        "    if not text_stripped:\n",
        "        return False\n",
        "\n",
        "    # 1. Pieds de page et numéros de page\n",
        "    if \"برنامج التدريس\" in text_stripped or \"نسخة تجريبية\" in text_stripped or re.fullmatch(r\"\\s*\\d+\\s*\", text_stripped):\n",
        "        return False\n",
        "\n",
        "    # 2. Titres (doivent être des correspondances exactes ou presque pour les blocs courts)\n",
        "    common_titles_keywords = [\n",
        "        \"قراءة نصوص قصيرة\", \"قراءة الفقرات البسيطة\", \"اللبنة\", \"التحدي\", \"توليف\",\n",
        "        \"قراءتي الأولى\", \"قراءتي الثانية\", \"قِراءَةُ نُصوصٍ قَصِيرَةٍ\",\n",
        "        \"قِراءَةُ الْفِقْراتِ الْبَسِيطَةِ\", \"اللّبِنَة\",\n",
        "        \"الْقِطَّةُ التَّائِهَةُ\", \"فَضْلُ الْبِذارِ\", \"صَبِيٌّ مُزْعِج\", \"رِسالَةُ عُصْفُورٍ\",\n",
        "        \"الصَّدِيقُ وَقْتَ الضَّيقِ\", \"الْقِطَّةُ الْفَنَّانَةُ\", \"دَرْسُ فِي الصَّبْرِ\",\n",
        "        \"بَيْتُ الْكَلْبِ\", \"أُمّي\", \"السُّلَحْفَاةُ الثَّرْثَارَةُ\", \"الْجَدْوَلُ الصَّغِيرُ\",\n",
        "        \"عَيْنُ الطَّائِرِ\", \"ثَمَنُ الشَّواءِ\", \"نصيحة غالية\", \"لِنُحَافِظ عَلَى الْأَزْهَارِ\",\n",
        "        \"الطَّفْلُ الْمَطَاطِيُّ\", \"مُكْرَه أَخاكَ ، لا بَطَل\", \"تُفَاحَةُ نيوتن\", \"لَيْسَ فِي كُلِّ مَرَّةٍ\",\n",
        "        \"حلق عالياً\"\n",
        "    ]\n",
        "    title_regex_patterns = [r\"^\\s*اللبنة\\s*\\d+\\s*$\", r\"^\\s*توليف\\s*\\d+\\s*$\", r\"^\\s*التحدي\\s*\\d*\\s*$\"] # Fin de ligne pour exactitude\n",
        "\n",
        "    for title_kw in common_titles_keywords:\n",
        "        if title_kw == text_stripped: return False\n",
        "    for pattern in title_regex_patterns:\n",
        "        if re.match(pattern, text_stripped, re.IGNORECASE): return False\n",
        "\n",
        "    # 3. Instructions d'exercices et questions\n",
        "    instruction_keywords_starters = [\n",
        "        \"اَقْرَا\", \"اُلاحِظُ\", \"اُكْمِلُ\", \"اُجِيبُ\", \"اَكْتُبُ\", \"اَصِلُ\", \"اَبْحَثُ\",\n",
        "        \"اَسْتَخْرِجُ\", \"صِلْ\", \"ضَعْ\", \"رَكِبْ\", \"رَكِّبْ\", \"اُعيدُ\", \"اُنْطِلاقاً\", \"أَقْرَأُ\",\n",
        "        \"أُلاحِظُ\", \"أُكْمِلُ\", \"أُجِيبُ\", \"أَكْتُبُ\", \"أَصِلُ\", \"أَبْحَثُ\", \"أَسْتَخْرِجُ\",\n",
        "        \"أُعيدُ قِراءَةَ النَّصِ\", \"اِسْتَخْرِجْ مِنَ النَّصِ\", \"أَقْرَأُ الْكَلِمَاتِ\", \"أَقْرَأُ النَّصَّ\",\n",
        "        \"أَقْرَأُ الْفَقَراتِ\"\n",
        "    ]\n",
        "    # Modèle: optionnel (numéro/lettre + puce) suivi d'un mot-clé d'instruction.\n",
        "    match_instruction_start = re.match(r\"^\\s*(?:\\d+|[\\u0621-\\u064A\\u0660-\\u0669a-zA-Z])\\s*[-–—.)]?\\s*(.*)\", text_stripped)\n",
        "    if match_instruction_start:\n",
        "        text_after_bullet = match_instruction_start.group(1).strip()\n",
        "        # Si le texte après la puce commence par un mot-clé d'instruction (ou si le mot-clé est le texte entier)\n",
        "        if any(text_after_bullet.startswith(keyword) for keyword in instruction_keywords_starters) or \\\n",
        "           any(keyword == text_after_bullet for keyword in instruction_keywords_starters):\n",
        "            return False\n",
        "        if len(text_after_bullet.split()) < 8 and text_after_bullet.endswith((\"؟\", \"?\", \":\")):\n",
        "            return False\n",
        "        # Cas comme \"أ -\" seul\n",
        "        if re.fullmatch(r\"^\\s*[\\u0621-\\u064A\\u0660-\\u0669a-zA-Z]\\s*[-–—.)]?\\s*$\", text_stripped):\n",
        "            return False\n",
        "        if len(text_after_bullet.split()) < 3 and not text_after_bullet: # Si après la puce c'est vide ou très court\n",
        "             return False\n",
        "\n",
        "\n",
        "    general_instructions_phrases = [\n",
        "        \"عَلي دَفْتَرِكَ\", \"في دَفْتَري\", \"ذاتِ الْمَعْ ني\", \"مِنَ الْكَلِم اتِ التّالِيَةِ\",\n",
        "        \"الْفِقْرَتَيْنِ وَأُجِيبُ عَنِ الْأَسْئِلَةِ\", \"أَسْئِلَةِ عَلَى دَفْتَرِي\", \"أَكْبَرَ عَدَد مِنَ الْجُمَلِ\"\n",
        "    ]\n",
        "    for instr_phrase in general_instructions_phrases:\n",
        "        # Si la phrase d'instruction constitue la majorité du texte et que le texte est court\n",
        "        if instr_phrase in text_stripped and len(text_stripped.split()) < (len(instr_phrase.split()) + 5) :\n",
        "            return False\n",
        "\n",
        "    # 4. Tableaux de vocabulaire / Listes de mots\n",
        "    table_keywords = [\"مَتي\", \"ذَ لِك\", \"ُحَيْث\", \"َكَيْف\", \"َّلَكِن\", \"ُاَقْرَا\", \"ِهَذِ ه\", \"حَت ي\",\n",
        "                      \"َمَع\", \"طِ فْل\", \"َمَدْ رَس ة\", \"َتَحْت\", \"ْهَل\", \"لِماذا\", \"اَلَّتي\", \"اُخْت\",\n",
        "                      \"اِمّْلء\", \"اِسْم\", \"اَلَّذي\", \"َفَوْق\", \"ْمَن\", \"في\", \"هَذا\", \"َاَيْن\",\n",
        "                      \"اَعْتَني\", \"مُشاكِس\", \"ُيَغْ ضَب\", \"مُنافَسَة\", \"ِاِجْت هاد\", \"عَزيمَة\"]\n",
        "\n",
        "    words_in_line = text_stripped.split()\n",
        "    word_count_in_stripped = len(words_in_line)\n",
        "\n",
        "    if word_count_in_stripped > 0 and word_count_in_stripped <= 15: # Lignes typiques des tableaux/listes\n",
        "        # Compter les mots séparés par des tirets (comme la liste اَعْتَني- مُشاكِس-)\n",
        "        if text_stripped.count('-') >= 2 and word_count_in_stripped > text_stripped.count('-') : # plus de 2 tirets\n",
        "             return False # Probablement une liste de mots avec tirets\n",
        "\n",
        "        table_keyword_count = sum(1 for kw in table_keywords if kw in words_in_line)\n",
        "        if table_keyword_count / word_count_in_stripped >= 0.6: # Si 60% des mots sont des keywords de tableau\n",
        "            if not text_stripped.endswith(('.', '!', '؟', ':')):\n",
        "                return False\n",
        "        elif word_count_in_stripped <= 5 and table_keyword_count >=2 : # Très court avec au moins 2 mots de tableau\n",
        "             if not text_stripped.endswith(('.', '!', '؟', ':')):\n",
        "                return False\n",
        "\n",
        "\n",
        "    # 5. Cas des exercices à compléter\n",
        "    if \"..........\" in text_stripped and word_count_in_stripped < 10:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def extract_text_blocks_from_pdf(pdf_path):\n",
        "    page_blocks_text = []\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            blocks_raw_text = page.get_text(\"blocks\", sort=True)\n",
        "            for block_info in blocks_raw_text:\n",
        "                if block_info[6] == 0:\n",
        "                    current_block_text = block_info[4]\n",
        "                    if current_block_text.strip():\n",
        "                        page_blocks_text.append(current_block_text.strip())\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur extraction blocs PDF {pdf_path}: {e}\")\n",
        "    return page_blocks_text\n",
        "\n",
        "def process_all_pdfs_for_contexts(pdf_dir, output_file):\n",
        "    all_initial_paragraphs_data = []\n",
        "\n",
        "    if not os.path.exists(pdf_dir):\n",
        "        print(f\"Le répertoire PDF '{pdf_dir}' n'existe pas.\")\n",
        "        return\n",
        "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(\".pdf\")]\n",
        "    if not pdf_files:\n",
        "        print(f\"Aucun fichier PDF dans '{pdf_dir}'.\")\n",
        "        return\n",
        "\n",
        "    for filename in pdf_files:\n",
        "        pdf_path = os.path.join(pdf_dir, filename)\n",
        "        print(f\"\\n--- Traitement du fichier : {filename} ---\")\n",
        "\n",
        "        raw_blocks = extract_text_blocks_from_pdf(pdf_path)\n",
        "\n",
        "        file_paragraph_id_counter = 0\n",
        "        for block_text_raw in raw_blocks:\n",
        "            normalized_block_text = normalize_arabic_text(block_text_raw)\n",
        "\n",
        "            # Chaque bloc est un candidat de paragraphe, les \\n internes au bloc sont préservés ici\n",
        "            # La division en sous-paragraphes se fera si le bloc contient \\n\\s*\\n+\n",
        "            # Mais la plupart des blocs de PyMuPDF sont déjà bien segmentés.\n",
        "\n",
        "            # Traiter le bloc normalisé comme un paragraphe candidat\n",
        "            # Si le bloc contient lui-même des séparateurs de paragraphe, les traiter\n",
        "            sub_paragraphs = re.split(r'\\n\\s*\\n+', normalized_block_text) # Sépare par un ou plusieurs sauts de ligne\n",
        "            for para_candidate in sub_paragraphs:\n",
        "                para_stripped = para_candidate.strip() # Enlève les espaces de début/fin seulement\n",
        "                if not para_stripped: continue\n",
        "\n",
        "                if is_narrative_paragraph(para_stripped) and len(para_stripped.split()) >= MIN_PARAGRAPH_WORD_COUNT_INITIAL:\n",
        "                    file_paragraph_id_counter += 1\n",
        "                    all_initial_paragraphs_data.append({\n",
        "                        \"source_pdf\": filename,\n",
        "                        \"original_id_in_file\": file_paragraph_id_counter,\n",
        "                        \"contexte_raw_lines\": para_stripped # Conserve les \\n pour la fusion\n",
        "                    })\n",
        "        print(f\"Nombre initial de paragraphes candidats pour {filename}: {file_paragraph_id_counter}\")\n",
        "\n",
        "    if not all_initial_paragraphs_data:\n",
        "        print(\"Aucun contexte narratif initial n'a été extrait.\")\n",
        "        return\n",
        "\n",
        "    # Logique de fusion\n",
        "    fused_paragraphs_output = []\n",
        "    global_id_counter = 0\n",
        "    if all_initial_paragraphs_data:\n",
        "        current_fused_item = dict(all_initial_paragraphs_data[0])\n",
        "        current_fused_item[\"contexte\"] = current_fused_item.pop(\"contexte_raw_lines\")\n",
        "\n",
        "        for i in range(1, len(all_initial_paragraphs_data)):\n",
        "            next_para_item = all_initial_paragraphs_data[i]\n",
        "            next_context_raw_lines = next_para_item[\"contexte_raw_lines\"]\n",
        "\n",
        "            can_fuse = (\n",
        "                current_fused_item[\"source_pdf\"] == next_para_item[\"source_pdf\"] and\n",
        "                not current_fused_item[\"contexte\"].strip().endswith(('.', '!', '؟', ':')) and # Ne fusionne pas si le courant finit bien\n",
        "                len(current_fused_item[\"contexte\"].split()) < FUSE_CURRENT_PARA_MAX_WORDS and\n",
        "                len(next_context_raw_lines.split()) < FUSE_NEXT_PARA_MAX_WORDS and\n",
        "                is_narrative_paragraph(next_context_raw_lines) # S'assurer que le suivant est aussi narratif\n",
        "            )\n",
        "\n",
        "            if can_fuse:\n",
        "                current_fused_item[\"contexte\"] += \"\\n\" + next_context_raw_lines\n",
        "            else:\n",
        "                final_context_text = clean_internal_newlines_and_final_spaces(current_fused_item[\"contexte\"])\n",
        "                if len(final_context_text.split()) >= MIN_FUSED_PARAGRAPH_WORD_COUNT: # Vérifier la longueur finale avant fusion\n",
        "                     # Seulement si après fusion ou seul, il respecte le seuil *final*\n",
        "                    if len(final_context_text.split()) >= MIN_FUSED_PARAGRAPH_WORD_COUNT :\n",
        "                        global_id_counter += 1\n",
        "                        current_fused_item[\"global_id\"] = global_id_counter\n",
        "                        current_fused_item[\"contexte\"] = final_context_text\n",
        "                        fused_paragraphs_output.append(current_fused_item)\n",
        "\n",
        "                current_fused_item = dict(next_para_item)\n",
        "                current_fused_item[\"contexte\"] = current_fused_item.pop(\"contexte_raw_lines\")\n",
        "\n",
        "        final_context_text_last = clean_internal_newlines_and_final_spaces(current_fused_item[\"contexte\"])\n",
        "        if len(final_context_text_last.split()) >= MIN_FUSED_PARAGRAPH_WORD_COUNT:\n",
        "            global_id_counter += 1\n",
        "            current_fused_item[\"global_id\"] = global_id_counter\n",
        "            current_fused_item[\"contexte\"] = final_context_text_last\n",
        "            fused_paragraphs_output.append(current_fused_item)\n",
        "\n",
        "    if not fused_paragraphs_output:\n",
        "        print(\"Aucun contexte narratif après fusion et filtrage final.\")\n",
        "        return\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for item in fused_paragraphs_output:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"\\n{len(fused_paragraphs_output)} contextes narratifs (après fusion) extraits et sauvegardés dans {output_file}\")\n",
        "\n",
        "# --- Exécution ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(PDF_DIRECTORY):\n",
        "        try:\n",
        "            os.makedirs(PDF_DIRECTORY)\n",
        "            print(f\"Dossier '{PDF_DIRECTORY}' créé. Veuillez y placer vos fichiers PDF arabes.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Erreur création dossier '{PDF_DIRECTORY}': {e}\")\n",
        "            exit()\n",
        "\n",
        "    process_all_pdfs_for_contexts(PDF_DIRECTORY, OUTPUT_CONTEXT_FILE)\n",
        "\n",
        "    if os.path.exists(OUTPUT_CONTEXT_FILE):\n",
        "        print(f\"\\n--- Premiers contextes extraits de {OUTPUT_CONTEXT_FILE} ---\")\n",
        "        count = 0\n",
        "        with open(OUTPUT_CONTEXT_FILE, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if count < 20:\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        print(f\"\\nSource: {data.get('source_pdf')}, Global_ID: {data.get('global_id')}\")\n",
        "                        print(f\"Contexte: {data.get('contexte')}\")\n",
        "                        print(\"-\" * 30)\n",
        "                        count += 1\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"Ligne malformée: {line.strip()}\")\n",
        "                else:\n",
        "                    break"
      ],
      "metadata": {
        "id": "Sc4XM_8_wZ-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}